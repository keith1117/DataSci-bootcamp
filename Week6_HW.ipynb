{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "Name: Keith Wang\n",
    "\n",
    "Email: hw3345@nyu.edu\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1215f-525a-4fd4-8653-842279e505da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "#1\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "\n",
    "def split_by_speaker(\n",
    "    speaker_ids: Iterable[str],\n",
    "    train_ratio=0.70, val_ratio=0.15, seed=2025\n",
    ") -> Dict[str, List[str]]:\n",
    "    \n",
    "    speaker_ids = np.array(list(speaker_ids))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(len(speaker_ids))\n",
    "    speaker_ids = speaker_ids[perm]\n",
    "\n",
    "    n = len(speaker_ids)\n",
    "    n_train = int(round(train_ratio * n))\n",
    "    n_val   = int(round(val_ratio   * n))\n",
    "    n_test  = n - n_train - n_val\n",
    "\n",
    "    splits = {\n",
    "        \"train\": list(speaker_ids[:n_train]),\n",
    "        \"val\":   list(speaker_ids[n_train:n_train+n_val]),\n",
    "        \"test\":  list(speaker_ids[n_train+n_val:]),\n",
    "    }\n",
    "    return splits\n",
    "\n",
    "#2\n",
    "from typing import Tuple\n",
    "def split_kilian_utterances(\n",
    "    kilian_ids: Iterable[str],  # e.g., utterance IDs for Kilian\n",
    "    train_ratio=0.80, val_ratio=0.10, seed=2025\n",
    ") -> Dict[str, List[str]]:\n",
    "   \n",
    "    arr = np.array(list(kilian_ids))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(len(arr))\n",
    "    arr = arr[perm]\n",
    "\n",
    "    n = len(arr)\n",
    "    n_train = int(round(train_ratio * n))\n",
    "    n_val   = int(round(val_ratio   * n))\n",
    "    splits = {\n",
    "        \"train\": list(arr[:n_train]),\n",
    "        \"val\":   list(arr[n_train:n_train+n_val]),\n",
    "        \"test\":  list(arr[n_train+n_val:])\n",
    "    }\n",
    "    return splits\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def load_state_dict(self, sd): pass\n",
    "    def state_dict(self): return {}\n",
    "\n",
    "def train_global(model, train_data, val_data, max_epochs=20, early_stop_patience=3):\n",
    "    best_sd, best_val = None, -np.inf\n",
    "    no_improve = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        # train_one_epoch(model, train_data)\n",
    "        # val_metric = evaluate(model, val_data)\n",
    "        val_metric = np.random.rand()\n",
    "        if val_metric > best_val:\n",
    "            best_val = val_metric; best_sd = model.state_dict(); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stop_patience:\n",
    "                break\n",
    "    model.load_state_dict(best_sd)\n",
    "    return model\n",
    "\n",
    "def finetune_kilian(model, kilian_train, kilian_val, max_epochs=10, lr=1e-4, freeze_backbone=True):\n",
    "    if freeze_backbone:\n",
    "        pass\n",
    "    best_sd, best_val = None, -np.inf\n",
    "    no_improve = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        # train_one_epoch(model, kilian_train, lr=lr)\n",
    "        # val_metric = evaluate(model, kilian_val)\n",
    "        val_metric = np.random.rand()\n",
    "        if val_metric > best_val:\n",
    "            best_val = val_metric; best_sd = model.state_dict(); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= 3:\n",
    "                break\n",
    "    model.load_state_dict(best_sd)\n",
    "    return model\n",
    "\n",
    "global_model = Model()\n",
    "global_model = train_global(global_model, train_data=\"global-train\", val_data=\"global-val\")\n",
    "personalized_for_kilian = finetune_kilian(global_model, kilian_train=\"kilian-train\", kilian_val=\"kilian-val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "# 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset from the prompt\n",
    "X = np.array([[1,2],[1,4],[5,4],   # positive\n",
    "              [3,1],[3,2]])        # negative\n",
    "y = np.array([+1,+1,+1,-1,-1], dtype=int)\n",
    "\n",
    "def grid_1nn_plot(X, y, k=1, step=0.05):\n",
    "    x1_min, x1_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    x2_min, x2_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xs = np.arange(x1_min, x1_max, step)\n",
    "    ys = np.arange(x2_min, x2_max, step)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    Z = np.zeros_like(xx, dtype=int)\n",
    "    for i in range(xx.shape[0]):\n",
    "        for j in range(xx.shape[1]):\n",
    "            q = np.array([xx[i,j], yy[i,j]])\n",
    "            Z[i,j] = knn_predict(X, y, q, k=k)\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, levels=[-1,0,1])\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], label=\"+1\")\n",
    "    plt.scatter(X[y==-1,0], X[y==-1,1], label=\"-1\")\n",
    "    plt.legend(); plt.title(\"1-NN decision regions\"); plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "grid_1nn_plot(X, y, k=1)\n",
    "\n",
    "test_points = np.array([[2,3],[4,3]])\n",
    "for q in test_points:\n",
    "    print(\"q=\", q, \" -> 1-NN:\", knn_predict(X, y, q, k=1))\n",
    "\n",
    "# 2\n",
    "X2_pos = np.array([[100,2],[100,4],[500,4]])\n",
    "X2_neg = np.array([[300,1],[300,2]])\n",
    "X2 = np.vstack([X2_pos, X2_neg])\n",
    "y2 = np.array([+1,+1,+1,-1,-1], dtype=int)\n",
    "q = np.array([500,1])\n",
    "\n",
    "# before scaling\n",
    "pred_before = knn_predict(X2, y2, q, k=1)\n",
    "\n",
    "# after scaling to [0,1] per feature\n",
    "mins = X2.min(axis=0); maxs = X2.max(axis=0)\n",
    "def minmax_scale(A): return (A - mins) / (maxs - mins + 1e-12)\n",
    "pred_after = knn_predict(minmax_scale(X2), y2, minmax_scale(q), k=1)\n",
    "\n",
    "print(\"P2(2) (500,1): before scaling ->\", pred_before, \"; after scaling ->\", pred_after)\n",
    "\n",
    "# 3\n",
    "def knn_predict_with_mask(X_train, y_train, x_query, mask, k=1):\n",
    "    # 1 for present feature, 0 for missing\n",
    "    M = mask.astype(float)\n",
    "    diffs = (X_train - x_query[None,:]) * M[None,:]\n",
    "    dists = np.linalg.norm(diffs, axis=1) / (M.sum() + 1e-12)  # normalize by #present dims\n",
    "    nn_idx = np.argpartition(dists, kth=k-1)[:k]\n",
    "    votes = y_train[nn_idx].sum()\n",
    "    return 1 if votes >= 0 else -1\n",
    "\n",
    "# example: query with x2 missing\n",
    "mask = np.array([1,0], dtype=int)\n",
    "q_missing = np.array([2.5, 0.0])  # second dim ignored by mask\n",
    "print(\"P2(3) with missing x2 ->\", knn_predict_with_mask(X, y, q_missing, mask, k=3))\n",
    "\n",
    "# 4\n",
    "rng = np.random.default_rng(0)\n",
    "X_high = np.hstack([X + rng.normal(scale=0.05, size=X.shape) for _ in range(100)])  # ~200D\n",
    "from numpy.linalg import svd\n",
    "Xc = X_high - X_high.mean(axis=0, keepdims=True)\n",
    "U,S,Vt = svd(Xc, full_matrices=False)\n",
    "X_pca2 = U[:, :2] * S[:2]     # top-2 PCs scores\n",
    "q_high = np.hstack([np.array([2.5,3.0]) for _ in range(100)])\n",
    "q_high = q_high + rng.normal(scale=0.05, size=q_high.shape)\n",
    "# project query to PC space\n",
    "q_pca2 = (q_high - X_high.mean(axis=0)) @ Vt[:2].T\n",
    "print(\"P2(4) PCA-then-1NN ->\", knn_predict(X_pca2, y, q_pca2, k=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "import numpy as np\n",
    "\n",
    "def sign(z: float) -> int:\n",
    "    return 1 if z >= 0 else -1  # tie -> +1\n",
    "\n",
    "def predict_linear(w: np.ndarray, x: np.ndarray) -> int:\n",
    "    return sign(float(w @ x))\n",
    "\n",
    "def training_error(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "    # X: shape (n, d) [augment with 1 if you want bias inside w]\n",
    "    # y: shape (n,) in {-1, +1}\n",
    "    # w: shape (d,)\n",
    "    \n",
    "    preds = np.array([predict_linear(w, xi) for xi in X])\n",
    "    wrong = np.count_nonzero(preds != y)\n",
    "    return wrong / len(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[10., -2.],   # positive\n",
    "              [12.,  2.]])  # negative\n",
    "y = np.array([+1, -1])\n",
    "w = np.array([0., 0.])\n",
    "\n",
    "sequence = [w.copy()]\n",
    "changed = True\n",
    "steps = 0\n",
    "while changed and steps < 50:\n",
    "    changed = False\n",
    "    for xi, yi in zip(X, y):\n",
    "        margin = yi * float(w @ xi)\n",
    "        if margin <= 0:\n",
    "            w = w + yi * xi\n",
    "            sequence.append(w.copy())\n",
    "            changed = True\n",
    "    steps += 1\n",
    "\n",
    "print(\"Weight sequence:\", sequence)\n",
    "print(\"Total updates:\", len(sequence)-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given update log: (x, y, count)\n",
    "updates = [\n",
    "    ((0, 0, 0, 0, 4), +1, 2),\n",
    "    ((0, 0, 6, 5, 0), +1, 1),\n",
    "    ((3, 0, 0, 0, 0), -1, 1),\n",
    "    ((0, 9, 3, 6, 0), -1, 1),\n",
    "    ((0, 1, 0, 2, 5), -1, 1),\n",
    "]\n",
    "\n",
    "w = np.zeros(5, dtype=int)\n",
    "for x, y, c in updates:\n",
    "    w += y * c * np.array(x, dtype=int)\n",
    "\n",
    "print(\"Final weight vector w =\", w.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 1\n",
    "X_pos = np.array([[2.0, 3.0],\n",
    "                  [3.0, 2.0],\n",
    "                  [4.0, 3.0]])\n",
    "X_neg = np.array([[0.0, 0.5],\n",
    "                  [1.0, 1.0],\n",
    "                  [1.0, 0.0]])\n",
    "X = np.vstack([X_pos, X_neg])\n",
    "y = np.array([+1]*len(X_pos) + [-1]*len(X_neg), dtype=int)\n",
    "\n",
    "# Augment features to include bias: x' = (x1, x2, 1)\n",
    "Xa = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "\n",
    "# 2\n",
    "w = np.zeros(3, dtype=float)  # (w1, w2, b)\n",
    "history = [] # store weights after each update\n",
    "\n",
    "max_epochs = 50\n",
    "for _ in range(max_epochs):\n",
    "    updated = False\n",
    "    for xi, yi in zip(Xa, y):\n",
    "        if yi * float(w @ xi) <= 0:\n",
    "            w = w + yi * xi\n",
    "            history.append(w.copy())\n",
    "            updated = True\n",
    "    if not updated:\n",
    "        break\n",
    "\n",
    "print(\"Number of updates:\", len(history))\n",
    "print(\"Final w (w1, w2, b):\", w)\n",
    "\n",
    "# 3\n",
    "out_dir = Path(\"perceptron_frames\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def plot_points(ax):\n",
    "    ax.scatter(X_pos[:, 0], X_pos[:, 1], label=\"+1\")\n",
    "    ax.scatter(X_neg[:, 0], X_neg[:, 1], label=\"-1\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_title(\"Perceptron update step\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    # fix view so boundaries are comparable across frames\n",
    "    ax.set_xlim(-1, 5)\n",
    "    ax.set_ylim(-0.5, 4)\n",
    "\n",
    "def plot_boundary(ax, w_vec):\n",
    "    # Boundary: w1*x + w2*y + b = 0  ->  y = -(w1/w2)*x - b/w2\n",
    "    xs = np.linspace(-1, 5, 200)\n",
    "    if abs(w_vec[1]) < 1e-12:\n",
    "        # vertical line x = -b/w1\n",
    "        x0 = -w_vec[2] / (w_vec[0] if abs(w_vec[0]) > 1e-12 else 1e-12)\n",
    "        ax.axvline(x=x0)\n",
    "    else:\n",
    "        ys = - (w_vec[0] / w_vec[1]) * xs - (w_vec[2] / w_vec[1])\n",
    "        ax.plot(xs, ys)\n",
    "\n",
    "frame_paths = []\n",
    "for i, wi in enumerate(history, start=1):\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_points(ax)\n",
    "    plot_boundary(ax, wi)\n",
    "    p = out_dir / f\"update_{i:02d}.png\"\n",
    "    fig.savefig(p, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    frame_paths.append(str(p))\n",
    "\n",
    "print(\"Saved frames:\", frame_paths[:3], \"... total:\", len(frame_paths))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
